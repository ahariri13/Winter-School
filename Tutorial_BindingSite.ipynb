{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We start by cloning the repository and installing the required packages for the ProteinShake dataset.\n",
        "\"\"\"\n",
        "# !pip install git+https://github.com/BorgwardtLab/proteinshake.git"
      ],
      "metadata": {
        "id": "u3rYJfoGQ4K5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "313118fc-5059-4ae9-f4c0-db5d70dc2280"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe start by cloning the repository and installing the required packages for the ProteinShake dataset.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Since we are using Google Colab, let's install the required packages for pytorch geometric.\n",
        "Details on the installation can be found on https://pytorch-geometric.readthedocs.io/en/2.5.2/notes/installation.html\n",
        "The pip commands below should only be run once in the notebook. Once the packages are installed, you can comment them out.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "# !pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4-0nMzD8RGX",
        "outputId": "0421b189-3d26-47d6-f32d-aabfdbfa827b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "import wandb\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from proteinshake import tasks as ps_tasks\n",
        "import torch_geometric.transforms as T\n",
        "from torch.utils.data import random_split\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, GINConv,global_mean_pool, ChebConv,global_add_pool\n",
        "import torch.nn as nn\n",
        "\n",
        "\"\"\"\n",
        "Use arguments for the main model parameters we want to try in this notebook\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--hidden', type=int, default=320, help='Latent Dimension')\n",
        "parser.add_argument('--seed', type=int, default=1234, help='Random Seed')\n",
        "parser.add_argument('--batch_size', type=int, default=100, help='Batch Size')\n",
        "parser.add_argument('--num_layers', type=int, default=4, help='Number of graph convolutional layers')\n",
        "parser.add_argument('--K', type=int, default=4, help='Number of hops')\n",
        "parser.add_argument('--backbone', type=str, default='GCN', help='Use GCN as backbone- otherwise SAGE')\n",
        "parser.add_argument('--lr', type=float, default=0.0005, help='Learning rate')\n",
        "args = parser.parse_args([])\n"
      ],
      "metadata": {
        "id": "HZ6ZxB63RmoG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the dataset and transform to graph format using k-nn**"
      ],
      "metadata": {
        "id": "hb6nTDO7HNrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Load the task and the dataset\"\"\"\n",
        "\n",
        "datapath = './data/ec'\n",
        "task = ps_tasks.BindingSiteDetectionTask(root=datapath)\n",
        "dset = task.dataset\n",
        "\n",
        "\"\"\"We convert the protein 3D structures to $\\epsilon$-graphs ($\\epsilon=8$ here):\"\"\"\n",
        "\n",
        "def transform(data):\n",
        "    data, protein_dict = data\n",
        "    data.y = task.target(protein_dict)\n",
        "    return data\n",
        "\n",
        "dset = dset.to_graph(eps=8.0).pyg(transform=transform)"
      ],
      "metadata": {
        "id": "9EwA8N89HNQf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's plot the first sample of the dataset to get an idea about the geometry of the protein graphs we are dealing with."
      ],
      "metadata": {
        "id": "vskmaEbv3fUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Sample test for visualization\n",
        "sample=dset[0]\n"
      ],
      "metadata": {
        "id": "UQTO5Der1bmG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vxlq9lR1wgq",
        "outputId": "205db6a3-82a3-451c-af1f-aa7269531696"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[416], edge_index=[2, 4064], edge_attr=[4064, 1], y=[416])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import networkx as nx\n",
        "# import matplotlib.pyplot as plt\n",
        "# from torch_geometric.data import Data\n",
        "# from torch_geometric.utils import to_networkx\n",
        "\n",
        "# # Convert to NetworkX graph\n",
        "# G = to_networkx(sample, to_undirected=True)\n",
        "\n",
        "# # Use force-directed layout (similar to Gephi's ForceAtlas)\n",
        "# pos = nx.spring_layout(G, seed=42, k=0.07)  # k controls spread\n",
        "\n",
        "# # Plot the graph\n",
        "# plt.figure(figsize=(8, 8))\n",
        "# nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=300, font_size=10)\n",
        "# plt.title(\"Visualization of the Expanded Protein Graph\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "4Quf3TnO1zmw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Is there something specific you realize about this sample visualization ? Maybe some constraint we discussed in the slides ? ðŸ§ **"
      ],
      "metadata": {
        "id": "Zfsh4n4F3tfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load train/val/test splits; We can now create data loaders for train/val/test sets provided by ProteinShake:"
      ],
      "metadata": {
        "id": "hDEss13hHlwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "batch_size = args.batch_size\n",
        "train_loader = DataLoader(Subset(dset, task.train_index), batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(Subset(dset, task.val_index), batch_size=batch_size,\n",
        "                        shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(Subset(dset, task.test_index), batch_size=batch_size,\n",
        "                         shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "czeR6-8aRv8v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the distribution of the number of nodes over all samples in the dataset**"
      ],
      "metadata": {
        "id": "m0o8pgshKixR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# # Compute the number of nodes for each graph\n",
        "# num_nodes = [data.num_nodes for data in dset]\n",
        "\n",
        "# # Plot the distribution\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# bins = range(0, max(num_nodes) + 100, 100)  # Create bins with an interval of 100\n",
        "# plt.hist(num_nodes, bins=bins, edgecolor='black', alpha=0.75)\n",
        "\n",
        "# # Add labels and title\n",
        "# plt.xlabel('Number of Nodes', fontsize=14)\n",
        "# plt.ylabel('Frequency', fontsize=14)\n",
        "# plt.title('Distribution of Number of Nodes in the dataset', fontsize=16)\n",
        "# plt.xticks(bins, rotation=45, ha='right')  # Rotate x-axis labels and align them to the right\n",
        "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "# plt.tight_layout()  # Adjust layout to prevent overlapping\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "diO_rzxKTM5v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Below we define the Graph Neural Network (GNN) that takes as input the protein graph samples constructed above and embeds them in a latent space. In the latter, the GNN learns the optimal weights to classify the nodes as belonging (or NOT) to the binding site.**"
      ],
      "metadata": {
        "id": "bjaZDGxm6Lx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Our model will start by first embedding the protein to a\n",
        "latent space of dimension \"hidden_dims\" which is a hyper-parameter to tune.\n",
        "\n",
        "Then a number \"num_layers\" of graph convolutional blocks is applied to the graph\n",
        "to learn over different neighborhoods of each node.\n",
        "\n",
        "Finally, the graph embeddings are sent to a linear classifier to predict\n",
        "whether each node belongs to the binding site or not.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class ProteinModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dims,num_layers,num_classes):\n",
        "        super(ProteinModel, self).__init__()\n",
        "\n",
        "        self.x_embedding = nn.Embedding(20, hidden_dims)\n",
        "\n",
        "        self.mlpRep = nn.Linear(hidden_dims,num_classes) ### Linear Classifier head for output\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "\n",
        "        if args.backbone=='GCN': ### In case we want to use GCNConv as backbone encoder\n",
        "          self.convs.append(GCNConv(hidden_dims, hidden_dims))\n",
        "          for _ in range(num_layers - 1):\n",
        "              self.convs.append(GCNConv(hidden_dims, hidden_dims))\n",
        "\n",
        "        elif args.backbone=='SAGE': ### In case we want to use SAGEConv as backbone encoder\n",
        "          self.convs.append(SAGEConv(hidden_dims, hidden_dims))\n",
        "          for _ in range(num_layers - 1):\n",
        "              self.convs.append(SAGEConv(hidden_dims, hidden_dims))\n",
        "\n",
        "    def forward(self, x, edge_index, batch,device):\n",
        "        x = self.x_embedding(x)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            ## Add batchnorm and relu except at last layer\n",
        "            x = nn.BatchNorm1d(x.size(1)).to(device)(x)\n",
        "            x = F.leaky_relu(x)\n",
        "            x = F.dropout(x, p=0.1, training=self.training)\n",
        "        classifier=self.mlpRep(x)\n",
        "\n",
        "        return classifier\n"
      ],
      "metadata": {
        "id": "EKS9iPohTlzh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z0LhfWJHDEe5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YLQn-H8sQx1z"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We start by building a GCN model with 5 layers and 256 hidden dimensions (Check args list above)\n",
        "\"\"\"\n",
        "num_classes=task.num_classes\n",
        "model=ProteinModel(args.hidden,args.num_layers,num_classes)\n",
        "# model=HNO()\n",
        "\n",
        "\"\"\"\n",
        "Build an optimizer and define the train and test function\n",
        "\"\"\"\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=args.lr\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# set device\n",
        "device = torch.device(torch.cuda.current_device()) \\\n",
        "        if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "def retrieve_sets(flattened_list, length_indices):\n",
        "    sets = []\n",
        "    start_index = 0\n",
        "    for length in length_indices:\n",
        "        end_index = start_index + length\n",
        "        subset = flattened_list[start_index:end_index]\n",
        "        sets.append(subset.detach().cpu().numpy())\n",
        "        start_index = end_index\n",
        "    return sets\n",
        "\n",
        "\n",
        "def train_epoch(model):\n",
        "    model.train()\n",
        "    model.cuda()\n",
        "    running_loss = 0.\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        size = len(batch.y)\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_hat=model(batch.x,batch.edge_index,batch.batch,device)\n",
        "\n",
        "        # Flatten the list of lists\n",
        "        flattened_list = [item for sublist in batch.y for item in sublist]\n",
        "\n",
        "        # Convert the flattened list into a tensor\n",
        "        tenso = torch.tensor(flattened_list)\n",
        "\n",
        "        loss = criterion(y_hat, tenso.cuda())#torch.Tensor(batch.y))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * size\n",
        "\n",
        "    n_sample = len(train_loader.dataset)\n",
        "    epoch_loss = running_loss / n_sample\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "\"\"\"ProteinShake provides an evaluation function for each task `task.evaluate(y_true, y_pred)`.\"\"\"\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    scoresAll=0\n",
        "\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        y_hat=model(batch.x,batch.edge_index,batch.batch,device)\n",
        "        y_pred = y_hat.argmax(-1)\n",
        "\n",
        "        length_indices=[]\n",
        "        scoresBatch=0\n",
        "        for m in range(len(batch.y)):\n",
        "          length_indices.append(len(batch.y[m]))\n",
        "\n",
        "        predictions=retrieve_sets(y_pred, length_indices)\n",
        "        for n in range(len(batch.y)):\n",
        "          scores = task.evaluate(batch.y[n], predictions[n])#['mcc']\n",
        "          scoresBatch+=scores['mcc']\n",
        "        scoresAll+=scoresBatch/len(batch.y)\n",
        "\n",
        "    #     y_true.append(batch.y.cpu())\n",
        "    #     y_pred.append(y_hat.cpu())\n",
        "\n",
        "    # y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "    # y_pred = torch.vstack(y_pred).numpy()\n",
        "    # y_pred = y_pred.argmax(-1)\n",
        "    # scores = task.evaluate(y_true, y_pred)\n",
        "    return scoresAll/(step+1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"## Training\"\"\"\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "epochs = 100 # we train only 20 epochs here, but more epochs may result in better performance.\n",
        "\n",
        "config = dict (\n",
        "  Changes=\"None\",\n",
        "  hidden_dim=args.hidden,\n",
        "  batch_size=args.batch_size,\n",
        "  learning_rate = args.lr,\n",
        "  seed = args.seed,\n",
        "  layers = args.num_layers\n",
        ")\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "project=\"Winter School Tutorial\",\n",
        "name=\"GCN\",\n",
        "config=config,\n",
        ")\n",
        "\n",
        "best_val_score = 0.0\n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    train_loss = train_epoch(model)\n",
        "    val_scores = eval_epoch(model, val_loader)\n",
        "    val_score = val_scores#['mcc']\n",
        "    postfix = {'train_loss': train_loss, 'val_acc': val_score}\n",
        "\n",
        "    wandb.log({\"Val Acc\": val_score})\n",
        "    wandb.log({\"Train Loss\": train_loss})\n",
        "    wandb.log({\"Epoch\": epoch})\n",
        "\n",
        "    pbar.set_postfix(postfix)\n",
        "\n",
        "    if val_score > best_val_score:\n",
        "        best_val_score = val_score\n",
        "        best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "model.load_state_dict(best_weights)\n",
        "\n",
        "\"\"\"## Testing the trained model\"\"\"\n",
        "\n",
        "task.evaluate\n",
        "\n",
        "test_scores = eval_epoch(model, test_loader)\n",
        "wandb.log({\"Test\": test_scores})\n",
        "print(test_scores)\n"
      ],
      "metadata": {
        "id": "qEYbj2BYTh2g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "9124a3b2-f2bb-4ca9-feaa-3bdb941a595e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahariri\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250210_133735-6n94b6ln</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ahariri/Winter%20School%20Tutorial/runs/6n94b6ln' target=\"_blank\">GCN</a></strong> to <a href='https://wandb.ai/ahariri/Winter%20School%20Tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ahariri/Winter%20School%20Tutorial' target=\"_blank\">https://wandb.ai/ahariri/Winter%20School%20Tutorial</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ahariri/Winter%20School%20Tutorial/runs/6n94b6ln' target=\"_blank\">https://wandb.ai/ahariri/Winter%20School%20Tutorial/runs/6n94b6ln</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|â–Ž         | 3/100 [00:45<24:23, 15.09s/it, train_loss=0.35, val_acc=0.369]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.backbone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRN7ShsX8iQ5",
        "outputId": "655e431c-b869-4914-c20b-ce1183c77926"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XvKEHsFREz2j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}