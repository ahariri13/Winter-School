{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We start by cloning the repository and installing the required packages for the ProteinShake dataset.\n",
        "\"\"\"\n",
        "# !pip install git+https://github.com/BorgwardtLab/proteinshake.git"
      ],
      "metadata": {
        "id": "u3rYJfoGQ4K5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7330767d-4532-4064-9bf4-ce256e320325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe start by cloning the repository and installing the required packages for the ProteinShake dataset.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Since we are using Google Colab, let's install the required packages for pytorch geometric.\n",
        "Details on the installation can be found on https://pytorch-geometric.readthedocs.io/en/2.5.2/notes/installation.html\n",
        "The pip commands below should only be run once in the notebook. Once the packages are installed, you can comment them out.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# ! pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "# !pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4-0nMzD8RGX",
        "outputId": "4c6b5290-e0c6-4c53-82af-cc9773e6db72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's starting by simply creating a PyG Data instance from a Point Cloud using K-nearest neighbors."
      ],
      "metadata": {
        "id": "qsMeTYl5NFnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.transforms import KNNGraph\n",
        "from torch_geometric.data import Data\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Necessary for 3D plotting\n",
        "\n",
        "\n",
        "def create_knn_point_cloud_data(num_points=100, k=5):\n",
        "    \"\"\"\n",
        "    Creates a random 3D point cloud of size `num_points`,\n",
        "    then builds a k-NN graph with `k` neighbors per node.\n",
        "    Returns a PyG Data object.\n",
        "    \"\"\"\n",
        "    knn_graph= ### ... Define Class\n",
        "\n",
        "\n",
        "    pos =  ## ... Generate random 3D points\n",
        "\n",
        "    data = ### ... Create data instance\n",
        "\n",
        "    data = # ... Build the k-NN graph\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def visualize_point_cloud(data, show_edges=False):\n",
        "    \"\"\"\n",
        "    Visualizes the point cloud in 3D.\n",
        "    If `show_edges` is True, also draws lines between connected points.\n",
        "    \"\"\"\n",
        "    # Convert PyTorch tensor to NumPy array for plotting\n",
        "    pos = data.pos.cpu().numpy()\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "    # Plot the points\n",
        "    ax.scatter(pos[:, 0], pos[:, 1], pos[:, 2],\n",
        "               c='b', marker='o', s=20, alpha=0.8)\n",
        "\n",
        "    if show_edges:\n",
        "        # edge_index is [2, num_edges]\n",
        "        edge_index = data.edge_index\n",
        "        num_edges = edge_index.shape[1]\n",
        "\n",
        "        # Loop through edges and draw lines between them\n",
        "        for i in range(num_edges):\n",
        "            start = edge_index[0, i].item()\n",
        "            end = edge_index[1, i].item()\n",
        "\n",
        "            # Points for the edge line\n",
        "            x_vals = [pos[start, 0], pos[end, 0]]\n",
        "            y_vals = [pos[start, 1], pos[end, 1]]\n",
        "            z_vals = [pos[start, 2], pos[end, 2]]\n",
        "\n",
        "            ax.plot(x_vals, y_vals, z_vals, c='r', alpha=0.3)\n",
        "\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "    plt.title('3D Point Cloud with k-NN Graph' if show_edges else '3D Point Cloud')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a point cloud Data object\n",
        "    data = create_knn_point_cloud_data(num_points=100, k=5)\n",
        "    # Visualize only points\n",
        "    visualize_point_cloud(data, show_edges=False)\n",
        "    # Visualize points with edges\n",
        "    visualize_point_cloud(data, show_edges=True)\n"
      ],
      "metadata": {
        "id": "nPy49-ibNRGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "import wandb\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from proteinshake import tasks as ps_tasks\n",
        "import torch_geometric.transforms as T\n",
        "from torch.utils.data import random_split\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, GINConv,global_mean_pool, ChebConv,global_add_pool\n",
        "import torch.nn as nn\n",
        "\n",
        "\"\"\"\n",
        "Use arguments for the main model parameters we want to try in this notebook\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--hidden', type=int, default=320, help='Latent Dimension')\n",
        "parser.add_argument('--seed', type=int, default=1234, help='Random Seed')\n",
        "parser.add_argument('--batch_size', type=int, default=100, help='Batch Size')\n",
        "parser.add_argument('--num_layers', type=int, default=7, help='Number of graph convolutional layers')\n",
        "parser.add_argument('--backbone', type=str, default='SAGE', help='Use GCN as backbone- otherwise SAGE')\n",
        "parser.add_argument('--lr', type=float, default=0.0005, help='Learning rate')\n",
        "args = parser.parse_args([])\n"
      ],
      "metadata": {
        "id": "HZ6ZxB63RmoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the dataset and transform to graph format using k-nn**"
      ],
      "metadata": {
        "id": "hb6nTDO7HNrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Load the task and the dataset\"\"\"\n",
        "\n",
        "datapath = './data/ec'\n",
        "task = ps_tasks.BindingSiteDetectionTask(root=datapath)\n",
        "dset = task.dataset\n",
        "\n",
        "\"\"\"We convert the protein 3D structures to $\\epsilon$-graphs ($\\epsilon=8$ here):\"\"\"\n",
        "\n",
        "def transform(data):\n",
        "    data, protein_dict = data\n",
        "    data.y = task.target(protein_dict)\n",
        "    return data\n",
        "\n",
        "dset = dset.to_graph(eps=8.0).pyg(transform=transform)"
      ],
      "metadata": {
        "id": "9EwA8N89HNQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Let's plot the first sample of the dataset to get an idea about the geometry of the protein graphs we are dealing with.**"
      ],
      "metadata": {
        "id": "vskmaEbv3fUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Sample test for visualization\n",
        "sample= ### ... Load first sample of the dataset to provide it for visualization below\n"
      ],
      "metadata": {
        "id": "UQTO5Der1bmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "id": "8vxlq9lR1wgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "G = ### ... Convert to networkx format\n",
        "\n",
        "pos = ###... Use spring layout with k=0.1 and seed=42 to visualize the graph\n",
        "\n",
        "# Plot the graph\n",
        "plt.figure(figsize=(8, 8))\n",
        "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=300, font_size=10)\n",
        "plt.title(\"Visualization of the Expanded Protein Graph\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Quf3TnO1zmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Is there something specific you realize about this sample visualization ? Maybe some constraint we discussed in the slides ? 🧠**"
      ],
      "metadata": {
        "id": "Zfsh4n4F3tfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load train/val/test splits; We can now create data loaders for train/val/test sets provided by ProteinShake:**"
      ],
      "metadata": {
        "id": "hDEss13hHlwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(Subset(dset, task.train_index), batch_size=args.batch_size,\n",
        "                          shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(Subset(dset, task.val_index), batch_size=args.batch_size,\n",
        "                        shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(Subset(dset, task.test_index), batch_size=args.batch_size,\n",
        "                         shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "czeR6-8aRv8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plot the distribution of the number of nodes over all samples in the dataset**"
      ],
      "metadata": {
        "id": "m0o8pgshKixR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Compute the number of nodes for each graph\n",
        "num_nodes = [data.num_nodes for data in dset]\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "bins = range(0, max(num_nodes) + 100, 100)  # Create bins with an interval of 100\n",
        "plt.hist(num_nodes, bins=bins, edgecolor='black', alpha=0.75)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Number of Nodes', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.title('Distribution of Number of Nodes in the dataset', fontsize=16)\n",
        "plt.xticks(bins, rotation=45, ha='right')  # Rotate x-axis labels and align them to the right\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "diO_rzxKTM5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Below we define the Graph Neural Network (GNN) that takes as input the protein graph samples constructed above and embeds them in a latent space. In the latter, the GNN learns the optimal weights to classify the nodes as belonging (or NOT) to the binding site.**"
      ],
      "metadata": {
        "id": "bjaZDGxm6Lx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Our model will start by first embedding the protein to a\n",
        "latent space of dimension \"hidden_dims\" which is a hyper-parameter to tune.\n",
        "\n",
        "Then a number \"num_layers\" of graph convolutional blocks is applied to the graph\n",
        "to learn over different neighborhoods of each node.\n",
        "\n",
        "Finally, the graph embeddings are sent to a linear classifier to predict\n",
        "whether each node belongs to the binding site or not.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class ProteinModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dims,num_layers,num_classes):\n",
        "        super(ProteinModel, self).__init__()\n",
        "\n",
        "        self.x_embedding = nn.Embedding(20, hidden_dims)\n",
        "\n",
        "        ### ... Build graph convolutional layers here\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, batch,device):\n",
        "        x = self.x_embedding(x)### This is an inital embedding to the correct latent space\n",
        "\n",
        "          ## Add the correspding number of graph convolutional layers you defined above\n",
        "          ## Add batchnorm and relu\n",
        "        classifier= ### ... Linear Classifier head for output\n",
        "\n",
        "        return classifier\n"
      ],
      "metadata": {
        "id": "EKS9iPohTlzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLQn-H8sQx1z"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We start by building a GCN model with x layers and x hidden dimensions (Check args list above)\n",
        "\"\"\"\n",
        "num_classes=task.num_classes\n",
        "model=ProteinModel(args.hidden,args.num_layers,num_classes)\n",
        "\n",
        "\"\"\"\n",
        "Build an optimizer and define the train and test function\n",
        "\"\"\"\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=args.lr\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# set device\n",
        "device = torch.device(torch.cuda.current_device()) \\\n",
        "        if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "def retrieve_sets(flattened_list, length_indices):\n",
        "    sets = []\n",
        "    start_index = 0\n",
        "    for length in length_indices:\n",
        "        end_index = start_index + length\n",
        "        subset = flattened_list[start_index:end_index]\n",
        "        sets.append(subset.detach().cpu().numpy())\n",
        "        start_index = end_index\n",
        "    return sets\n",
        "\n",
        "\n",
        "def train_epoch(model):\n",
        "    model.train()\n",
        "    model.cuda()\n",
        "    running_loss = 0.\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        size = len(batch.y)\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_hat=model(batch.x,batch.edge_index,batch.batch,device)\n",
        "\n",
        "        # Flatten the list of lists\n",
        "        flattened_list = [item for sublist in batch.y for item in sublist]\n",
        "\n",
        "        # Convert the flattened list into a tensor\n",
        "        tenso = torch.tensor(flattened_list)\n",
        "\n",
        "        loss = criterion(y_hat, tenso.cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * size\n",
        "\n",
        "    n_sample = len(train_loader.dataset)\n",
        "    epoch_loss = running_loss / n_sample\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "\"\"\"ProteinShake provides an evaluation function for each task `task.evaluate(y_true, y_pred)`.\"\"\"\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    scoresAll=0\n",
        "\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        y_hat=model(batch.x,batch.edge_index,batch.batch,device)\n",
        "        y_pred = y_hat.argmax(-1)\n",
        "\n",
        "        length_indices=[]\n",
        "        scoresBatch=0\n",
        "        for m in range(len(batch.y)):\n",
        "          length_indices.append(len(batch.y[m]))\n",
        "\n",
        "        predictions=retrieve_sets(y_pred, length_indices)\n",
        "        for n in range(len(batch.y)):\n",
        "          scores = task.evaluate(batch.y[n], predictions[n])#['mcc']\n",
        "          scoresBatch+=scores['mcc']\n",
        "        scoresAll+=scoresBatch/len(batch.y)\n",
        "\n",
        "    #     y_true.append(batch.y.cpu())\n",
        "    #     y_pred.append(y_hat.cpu())\n",
        "\n",
        "    # y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "    # y_pred = torch.vstack(y_pred).numpy()\n",
        "    # y_pred = y_pred.argmax(-1)\n",
        "    # scores = task.evaluate(y_true, y_pred)\n",
        "    return scoresAll/(step+1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.backbone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dh34k7MnXq4g",
        "outputId": "08d2a4d0-c1e0-47fa-d05f-c6774ce106da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SAGE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"## Training\"\"\"\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "epochs = 100 # we train only 20 epochs here, but more epochs may result in better performance.\n",
        "\n",
        "config = dict (\n",
        "  Changes=\"None\",\n",
        "  hidden_dim=args.hidden,\n",
        "  batch_size=args.batch_size,\n",
        "  learning_rate = args.lr,\n",
        "  seed = args.seed,\n",
        "  layers = args.num_layers\n",
        ")\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "project=\"Winter School Tutorial\",\n",
        "name=args.backbone,\n",
        "config=config,\n",
        ")\n",
        "\n",
        "best_val_score = 0.0\n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    train_loss = train_epoch(model)\n",
        "    val_scores = eval_epoch(model, val_loader)\n",
        "    val_score = val_scores#['mcc']\n",
        "    postfix = {'train_loss': train_loss, 'val_acc': val_score}\n",
        "\n",
        "    wandb.log({\"Val Acc\": val_score})\n",
        "    wandb.log({\"Train Loss\": train_loss})\n",
        "    wandb.log({\"Epoch\": epoch})\n",
        "\n",
        "    pbar.set_postfix(postfix)\n",
        "\n",
        "    if val_score > best_val_score:\n",
        "        best_val_score = val_score\n",
        "        best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "model.load_state_dict(best_weights)\n",
        "\n",
        "\"\"\"## Testing the trained model\"\"\"\n",
        "\n",
        "task.evaluate\n",
        "\n",
        "test_scores = eval_epoch(model, test_loader)\n",
        "wandb.log({\"Test\": test_scores})\n",
        "print(test_scores)\n"
      ],
      "metadata": {
        "id": "qEYbj2BYTh2g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "b7852a81-010c-4ce4-ded1-5eb0e49248bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahariri\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250210_145910-86ar0qm7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ahariri/Winter%20School%20Tutorial/runs/86ar0qm7' target=\"_blank\">SAGE</a></strong> to <a href='https://wandb.ai/ahariri/Winter%20School%20Tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ahariri/Winter%20School%20Tutorial' target=\"_blank\">https://wandb.ai/ahariri/Winter%20School%20Tutorial</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ahariri/Winter%20School%20Tutorial/runs/86ar0qm7' target=\"_blank\">https://wandb.ai/ahariri/Winter%20School%20Tutorial/runs/86ar0qm7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [32:22<00:00, 19.42s/it, train_loss=0.121, val_acc=0.746]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.703429291462553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You have trained your first GNN model on Protein, congrats ! Now that this task is done, re-run the above cells by changing the backbone GNN. For instance, if you started by training a GCN backbone, change it now to a SAGE backbone and see how the plots compare on WandB.**"
      ],
      "metadata": {
        "id": "e_W8qRgwgVE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Now try increasing the number of layers for each backbone, how do the results change ?**"
      ],
      "metadata": {
        "id": "kOs6CGMzgvVv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XvKEHsFREz2j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}